{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 Setting the Stage & Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Unless you try to do something beyond what you have already mastered, you will never grow.â€ ~ \n",
    "Ronald E. Osborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline for this notebook  \n",
    "\n",
    "1. What is Data Analytics? ðŸ  \n",
    "2. What is a Data Analyst?\n",
    "3. What is the Data Analytics Cycle?\n",
    "4. Small, Medium, and Big Data\n",
    "5. Problem Definition\n",
    "6. Data Gathering\n",
    "7. Questions\n",
    "\n",
    "In this section, we will be covering 2 of the most important stages of any data analytics process: problem definition and data gathering. We will begin by talking about what data analytics is, what is a data analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Data Analytics? ðŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Analytics Cycle](../images/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is a Data Analyst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Analytics Cycle](../images/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![solving](../images/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As data detectives, we want to make sure we have at least a loosly define outline of what our projects involving data might look like. In particular, we want to be careful with those involving large amounts of data since errors can, at the very least, be very time consuming, and, at worst, very expensive.\n",
    "\n",
    "For our task, we are currently sick and tired of COVID and we want to start planning our next vacation. More specifically, we would love to scratch some countries off our bucket list, but, since this can be quite costly, we should start by figuring out where are we going, where are we staying, and what kind of prices are we looking at if we decide to go there?\n",
    "\n",
    "Since hotels are expensive, we thought we would give Airbnb a try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gathering Data](../images/9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using data scraped from a scraping tool called, [Inside Airbnb](http://insideairbnb.com/index.html). Yes, we will be scraping a bit of data from the scraper itsef. More specifically, we will be taking the skeleton (an html version of the website), downloading it, and then extracting all of the links that will help us get the data from it.\n",
    "\n",
    "We will start by importing the following packages to help us get the data we need.\n",
    "\n",
    "- `os`\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `requests`\n",
    "- `bs4`\n",
    "- `wget`\n",
    "- `glob`\n",
    "- `urllib`\n",
    "- `dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import wget\n",
    "import dask\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import urllib\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be creating several directories, the first thing we will do is to assign a path to the directory where all of our data will go into and come out from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create a function that takes in a existing path as a starting point and many additional directory names that we might want/need to create along the course of this tutorial. In addition, our function will check whether the directory we are trying to create exists or not, then combine all arguments into one directory and return such directory.\n",
    "\n",
    "You might have already seen the `*args` parameter in a function a few times already while using Python. What this does is that it gives us the ability to provide multiple arguments to a function without explicitely adding them to the construction of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_or_add(old_path, *args):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function will help us check whether one or more directories exists, and\n",
    "    if they don't exist, it will create, combine, and return a new directory.\n",
    "    \"\"\"\n",
    "        \n",
    "    if not os.path.exists(os.path.join(old_path, *args)):\n",
    "        os.makedirs(os.path.join(old_path, *args))\n",
    "\n",
    "    return os.path.join(old_path, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Python's `requests` module to send a request to Inside Airbnb, use our path creation function to add this html file to a directory called, `html_data`, and then save the file as text using a context manager construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_data = requests.get('http://insideairbnb.com/get-the-data.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_4_source = check_or_add(path, 'raw_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_4_source, 'insideairbnb.html'), 'w') as html:\n",
    "    html.write(web_data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the path to our new file, plus the name of the file we just saved, to a variable called `html_doc`. We will then read it back in as bytes, and parse the document using `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = os.path.join(path_4_source, 'insideairbnb.html')\n",
    "html_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html_doc, 'r') as file: \n",
    "    soup = BeautifulSoup(file, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BeautifulSoup` will allow us to extract the links we need without much hassle. While we could figure out a way to get the exact links we need,  maybe with a regular expression or a similar approach, we will extract all links at this stage by parsing the html file and taking out the links we need. For this, we will use a Python list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_links = [link.get('href') for link in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_links[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(list_of_links)} links. Wow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the files we need are the ones that end in `listings.csv.gz` and, to extract them, (or filter out those we don't want), we can take advantage pandas' many string methods. Let's begin by converting our list into a pandas series, which can also be referred to as a 1 dimensional array with a maleable index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_list = pd.Series(list_of_links, name='links')\n",
    "\n",
    "our_list.head() # let's examine the first five rows of our new pandas Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to get rid of `NaN` values, grab the listings links, and filter out those likns we don't with a mask. We will also reset the index just because it is nice to have values that start from 0 for our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_list.dropna(inplace=True) # drop NaN's and keep the changes\n",
    "\n",
    "condition = our_list.str.endswith('/listings.csv.gz') # let's find the listings we need\n",
    "\n",
    "files_we_want = our_list[condition].reset_index(drop=True) # filter out what we don't need and reset the index\n",
    "\n",
    "files_we_want.head() # make sure everything when through as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the links we need, let's go ahead and examine how many we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_we_want.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's certainly still a lot of files to download (but at least is not 20k), so how about we have a look at how many files we have per country and, where possible, per city. Since we can imagine places such as the US, the UK, and Australia having multiple cities with people doing business through Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = files_we_want.str.split('/').str.get(3)\n",
    "\n",
    "unique_countries = countries.unique()\n",
    "unique_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in unique_countries:\n",
    "    print(f\"{country.title()} has ------> {len(files_we_want[countries == country])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Find out how many cities are represented in our dataset, and print the country, city, and how many files for that city do we have. Name the list of unique cities, `unique_cities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers below! Don't peak! ðŸ‘€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities = files_we_want.str.split('/').str.get(5)\n",
    "unique_cities = cities.unique()\n",
    "unique_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for city in unique_cities:\n",
    "    print(f\"{city.title()} has ------> {len(files_we_want[cities == city])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time for us to pick a country or city for our analysis.\n",
    "\n",
    "### Let's pick a countries and cities to visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_country = 'japan'\n",
    "my_country2 = 'belgium'\n",
    "my_country3 = 'germany'\n",
    "my_city = 'cape-town'\n",
    "my_city2 = 'sydney'\n",
    "my_city3 = 'washington-dc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you forget the amount of files available in each country and/or city when trying to come up with a decision, you can check them individually with the following function. There is also a table with more information coming up soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len_files(country_city):\n",
    "    \n",
    "    if country_city in unique_countries:\n",
    "        \n",
    "        condition = files_we_want.str.contains(country_city)\n",
    "        data_we_need = files_we_want[condition]\n",
    "        \n",
    "        return len(data_we_need)\n",
    "    \n",
    "    elif country_city in unique_cities:\n",
    "        \n",
    "        condition = files_we_want.str.contains(country_city)\n",
    "        \n",
    "        data_we_need = files_we_want[condition]\n",
    "        \n",
    "        return len(data_we_need)\n",
    "    \n",
    "    else:\n",
    "        print(\"Sorry, your country or city is not on the list or it was misspelled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{my_country.title()} has {check_len_files(my_country)} files\")\n",
    "print(f\"{my_country2.title()} has {check_len_files(my_country2)} files\")\n",
    "print(f\"{my_country3.title()} has {check_len_files(my_country3)} files\")\n",
    "print(f\"{my_city.title()} has {check_len_files(my_city)} files\")\n",
    "print(f\"{my_city2.title()} has {check_len_files(my_city2)} files\")\n",
    "print(f\"{my_city3.title()} has {check_len_files(my_city3)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is one of the most important functions in the whole notebook as it is the one that is going to allow us to get the data we need into our computers.\n",
    "\n",
    "The function takes in the following arguments:\n",
    "- `urls` --> This is strictly a pandas series with the list of urls we need\n",
    "- `country_city` --> This would the country you want to get data for\n",
    "- `path_to_files` --> This is where the data will be downloaded to\n",
    "- `country_city_unique` --> This is the list of countries or cities where Airbnb operates in\n",
    "- `unique_num` --> If you do not need all files, you can specify how many you need. Default is all\n",
    "\n",
    "And it operates as follows:\n",
    "\n",
    "1. It first checks whether the country you have picked is in the list of unique countries\n",
    "2. Then it creates a boolean array (aka a mask)\n",
    "3. Passes it through our pandas series containing the urls to filter out the countries you don't need\n",
    "4. Then it downloads the files you want and\n",
    "5. Saves them into a new folder it creates called `raw_data` in the path you provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_me_specific_data(urls, country_city, path_to_files, country_city_unique, unique_num = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # we go over every country\n",
    "    if country_city in country_city_unique:\n",
    "        \n",
    "        # check whether it exists in our list of urls and create a mask\n",
    "        condition = urls.str.contains(country_city)\n",
    "        \n",
    "        # we pass that mask to our pandas series\n",
    "        data_we_need = urls[condition]\n",
    "        \n",
    "        # create a new directory for the raw data\n",
    "        new_dir = check_or_add(path_to_files, country_city + '_data', 'raw_data')\n",
    "        \n",
    "        # we first check if a unique number of files was specified\n",
    "        if unique_num:\n",
    "            \n",
    "            num = 0\n",
    "            \n",
    "            # loop until we reach that point\n",
    "            while num < unique_num:\n",
    "                \n",
    "                # we first try to download the file with wget\n",
    "                try:\n",
    "                    # if wget doesn't work, we try with urllib\n",
    "                    wget.download(data_we_need.iloc[num], os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                except:\n",
    "                    try:\n",
    "                        urllib.request.urlretrieve(data_we_need.iloc[num], os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                    except:\n",
    "                        continue\n",
    "                num += 1\n",
    "        else:\n",
    "            \n",
    "            # iterate over the links we want\n",
    "            for num, data in enumerate(data_we_need):\n",
    "                \n",
    "                # we first try to download the file with wget\n",
    "                try:\n",
    "                    # if wget doesn't work, we try with urllib\n",
    "                    wget.download(data, os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                except:\n",
    "                    try:\n",
    "                        # if urllib doesn't work, we move on to the next one\n",
    "                        urllib.request.urlretrieve(data, os.path.join(new_dir, f'{country_city}_{num}.csv.gz'))\n",
    "                    except:\n",
    "                        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function should not be used in this tutorial but is here for reference. What it does is that it will get all dowloadable files from Inside Airbnb in a similar fashion as with the previous formula.\n",
    "\n",
    "```python\n",
    "def get_me_all_data(urls, path_to_files, countries_unique):\n",
    "    \"\"\"\n",
    "    NOTE: Only use this function if you intend to download all of the data.\n",
    "    \n",
    "    Arguments:\n",
    "    urls: pandas series with the links to iterate over\n",
    "    path_to_files: path where you would like to save your files at\n",
    "    countries_unique: iterable with the countries where Airbnb operates\n",
    "    \n",
    "    \"\"\"\n",
    "    for country in countries_unique: # we go over every country\n",
    "        \n",
    "        condition = urls.str.contains(country) # create a mask for it\n",
    "        \n",
    "        data_we_need = urls[condition] # we pass that mask to our pandas series\n",
    "        \n",
    "        new_dir = check_or_add(path_to_files, country, 'raw_data') # create a new directory for the raw data\n",
    "        \n",
    "        for num, data in enumerate(data_we_need): # iterate over the links we want\n",
    "        \n",
    "            try: # we first try to download the file with wget\n",
    "                wget.download(data, os.path.join(new_dir, f'{country}_{num}.csv.gz'))\n",
    "            except:\n",
    "                try: # if wget doesn't work, we try with urllib\n",
    "                    urllib.request.urlretrieve(data, os.path.join(new_dir, f'{country}_{num}.csv.gz'))\n",
    "                except:\n",
    "                    continue # if urllib doesn't work, we move on to the next one\n",
    "```\n",
    "\n",
    "Let's put our new function to use and get the first batch of data we will be using. In honor to our host, we will be picking Japan as our first country,\n",
    "\n",
    "Here is a table with the countries, the amount of files available, the total size of the uncompressed and the compressed files, and the average size per file. The recommended way to pick a country and the amount of files you should download is as follows:\n",
    "1. Pick a reasonable GB size for your batch (somewhere between 2 and 4 GB should be perfect).\n",
    "2. Pick a country.\n",
    "3. If the amount of files in that country don't amount to the size you choose in step 1, pick another country or pick multiple countries until you have the desired amount of GB.\n",
    "4. If you want pick multiple countries but the total size of one or more of them is too large for what you think your computer can manage, divide the total GB size of that country by the GB space you have left and that would be the amount of files you should choose.\n",
    "5. Use the `get_me_specific_data()` function with the appropriate parameters and wait for a bit.\n",
    "\n",
    "\n",
    "| Country         | # of Cities | # of Files | GB Size Compressed  | GB Size Decompressed|\n",
    "|:----------------|:------------|:-----------|:--------------------|:--------------------|\n",
    "| The-Netherlands |     1       |     58     |        851 M        |        3.6 G        |\n",
    "| Belgium         |     3       |     83     |        245 M        |        1.0 G        |\n",
    "| United-States   |    28       |    859     |        8.4 G        |       35.0 G        |\n",
    "| Greece          |     4       |     82     |        902 M        |        3.8 G        |\n",
    "| Spain           |     9       |    259     |        2.7 G        |       12.0 G        |\n",
    "| Australia       |     7       |    233     |        2.6 G        |       11.0 G        |\n",
    "| China           |     3       |     57     |        1.1 G        |        4.9 G        |\n",
    "| Belize          |     1       |     15     |         38 M        |        180 M        |\n",
    "| Italy           |    10       |    246     |        4.0 G        |       16.0 G        |\n",
    "| Germany         |     2       |     63     |        894 M        |        3.6 G        |\n",
    "| France          |     3       |    117     |        3.1 G        |       13.0 G        |\n",
    "| United-Kingdom  |     5       |    125     |        2.7 G        |       11.0 G        |\n",
    "| Argentina       |     1       |     14     |        272 M        |        1.1 G        |\n",
    "| South-Africa    |     1       |     24     |        452 M        |        1.9 G        |\n",
    "| Denmark         |     1       |     27     |        505 M        |        2.2 G        |\n",
    "| Ireland         |     2       |     45     |        550 M        |        2.3 G        |\n",
    "| Switzerland     |     2       |     86     |        200 M        |        858 M        |\n",
    "| Turkey          |     1       |     25     |        275 M        |        1.2 G        |\n",
    "| Portugal        |     2       |     56     |        879 M        |        3.7 G        |\n",
    "| Mexico          |     1       |     16     |        279 M        |        1.1 G        |\n",
    "| Canada          |     7       |    191     |        1.4 G        |        6.0 G        |\n",
    "| Norway          |     1       |     26     |        156 M        |        663 M        |\n",
    "| Czech-Republic  |     1       |     25     |        317 M        |        1.3 G        |\n",
    "| Brazil          |     1       |     27     |        731 M        |        2.9 G        |\n",
    "| Chile           |     1       |      5     |         52 M        |        232 M        |\n",
    "| Singapore       |     1       |     16     |        102 M        |        516 M        |\n",
    "| Sweden          |     1       |     25     |        129 M        |        561 M        |\n",
    "| Taiwan          |     1       |     25     |        281 M        |        1.1 G        |\n",
    "| Japan           |     1       |     16     |        248 M        |        1.2 G        |\n",
    "| Austria         |     1       |     52     |        433 M        |        1.8 G        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put our function to use and get the data we need for our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "get_me_specific_data(files_we_want, my_country, path, unique_countries)\n",
    "get_me_specific_data(files_we_want, my_country2, path, unique_countries, 12)\n",
    "get_me_specific_data(files_we_want, my_country3, path, unique_countries, 12)\n",
    "get_me_specific_data(files_we_want, my_city, path, unique_cities, 12)\n",
    "get_me_specific_data(files_we_want, my_city2, path, unique_cities, 12)\n",
    "get_me_specific_data(files_we_want, my_city3, path, unique_cities, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the data we have gathered so far to see if we what we got back from Inside Airbnb. Since pandas has a nice compression parameter, we will not worry about decompressing our files with other tools and use pandas' in next few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = check_or_add(path, my_country + '_data', 'raw_data') # let's add our new raw_data path to a variable\n",
    "file_num = 5 # pick a number for the file you want to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(raw_files, f'{my_country}_{file_num}.csv.gz'), compression='gzip', low_memory=False, encoding='utf-8')\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at how many files we downloaded our first country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Amount of files we downloaded for {my_country} --> {len(os.listdir(raw_files))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain globbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(path, '*_data', 'raw_data', '*.csv.gz'))\n",
    "len(files), files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick example of dask delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(data, path_out, new_dir, country, nums):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data, compression='gzip',  low_memory=False, encoding='utf-8')\n",
    "    \n",
    "    df.to_csv(os.path.join(check_or_add(path_out, country + '_data', new_dir), f'{country}_{nums}.csv'), index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Done Reading and Saving file {nums}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe show one with delayed and the other without it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "\n",
    "for num, file in enumerate(files):\n",
    "    \n",
    "    if my_country in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country=my_country, nums=num)\n",
    "        results.append(result)\n",
    "        \n",
    "    elif my_country2 in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country=my_country2, nums=num)\n",
    "        results.append(result)\n",
    "        \n",
    "    elif my_country3 in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country=my_country3, nums=num)\n",
    "        results.append(result)\n",
    "        \n",
    "    elif my_city in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country=my_city, nums=num)\n",
    "        results.append(result)\n",
    "        \n",
    "    elif my_city2 in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country=my_city2, nums=num)\n",
    "        results.append(result)\n",
    "\n",
    "    elif my_city3 in file:\n",
    "        result = dask.delayed(get_csv_files)(data=file, path_out=path, new_dir='csv_files', country=my_city3, nums=num)\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_done = [result.compute() for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check that you have the correct amount of decompressed files with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = glob(os.path.join(path, '*_data', 'csv_files', '*.csv'))\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome Work! Now to Clean and Reshape our Data!\n",
    "\n",
    "![Cleaning](https://media.giphy.com/media/RjpE964WUAE5a/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciPy20JP",
   "language": "python",
   "name": "dask_tutorial_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
